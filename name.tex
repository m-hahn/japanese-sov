\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{tikz-dependency}

  \usepackage{natbib}
  \bibliographystyle{plainnat}

\usepackage{CJKutf8}

%  \usepackage{natbib}
 % \bibliographystyle{plainnat}


\pagestyle{plain}

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

% Use the lineno option to display guide line numbers if required.

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}





\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}


\usepackage{amsthm}
 


\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
\newcommand{\key}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand{\rljf}[1]{{\color{blue}[rljf: #1]}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}
\newcommand{\japanese}[1]{\begin{CJK}{UTF8}{min}#1\end{CJK}}


\title{Dependency Length Minimization and Basic Word Order in Japanese}
\author{Michael Hahn \\ Stanford University \\ mhahn2@stanford.edu}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
DLM widely supported, but appears to be at odds with the high typological frequency of SOV and VSO orders.
We report a corpus-based study of Japanese that tests this.
\end{abstract}

\section{Introduction}

There has been a suggestion that the typological distribution of word orders reflects a pressure towards efficiency of on-line syntactic processing.

Prominent among these proposals is the idea that syntactic structures are easier to process if syntactically related words are closer together in the surface string.

\cite{rijkhoff-word-1986} proposed a principle of \emph{Head Proximity}, arguing that, among other things, head nouns tend to be close to the verbs governing them.
\cite{hawkins2014crosslinguistic} proposes a similar principle of \emph{Domain Minimization}, arguing that languages minimize the distances at which syntactic relations are established in a sentence.
The principle of \emph{Dependency Length Minimization} \citep{temperley2018minimizing} is a formalization of these ideas in terms of dependency grammar.

Dependency Length Minimization has been argued to explain Greenberg's harmonic correlations \citep{rijkhoff-word-1986, hawkins1994performance}, and certain kinds of exceptions, such as why pronominal objects pattern differently from nominal objects \citep{dryer1992greenbergian}.
Hawkins (\citep{hawkins2004efficiency, hawkins2007processing, hawkins2014crosslinguistic}) has assembled further evidence in favor of 

Despite the success of Dependency Length Minimization, it appears to be at odds with the prevalence of SOV order:

In a sentence such as

\begin{centering}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
          She \& met \& friends  \\
   \end{deptext}
        %   \deproot{3}{ROOT}
   \depedge{2}{1}{subject}
   \depedge{2}{3}{object}
   %\depedge[arc angle=50]{7}{6}{ATT}
\end{dependency}
\end{centering}

the overall dependency length is minimized by the English order, compared to the Japanese order

\begin{centering}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
          \japanese{彼女は} \& \japanese{友達に} \& \japanese{会った}  \\
          kanojo-wa \& tomodachi-ni \& atta \\ 
          she-\textsc{subj} \& friends-\textsc{obj} \& met \\
   \end{deptext}
        %   \deproot{3}{ROOT}
   \depedge{3}{1}{subject}
   \depedge{3}{2}{object}
   %\depedge[arc angle=50]{7}{6}{ATT}
\end{dependency}
\end{centering}


This paper examines this claim.

First, does the above claim remain true if we consider sentences more complex than the three-word one above?
In particular, does it remain true if we consider the average dependency length over the distribution of sentences as found in a corpus?

We will address this question by comparing the dependency length of actual Japanese orderings with counterfactual orderings exhibiting SVO order.

Second, what is the situation if we consider a more direct formalization of online memory limitations?


\section{Experiment 1: Comparison of Dependency Length}

\subsection{Method}

\paragraph{Japanese}

Functional Heads:

cc: \begin{CJK}{UTF8}{min}と, や\end{CJK} 

case: doesn't apply to verbs\footnote{https://universaldependencies.org/ja/dep/case.html}

cop: \begin{CJK}{UTF8}{min}だ\end{CJK} 

mark: \begin{CJK}{UTF8}{min}と, か\end{CJK} 

Method for bringing S to the other side: sorting it after the other dependents, as these are short (auxiliaries etc).

\paragraph{English}

\subsection{Results}

\begin{center}
\begin{tabular}{lllllllll}
                    &                        & O,S same side   & O, S different sides    \\ \hline\hline
& Japanese                & 51.81 & 53.24 \\ % (excluding aux)
\end{tabular}
\end{center}


%TODO

%- COMPARISON TO ENGLISH

%- STATISTICAL INFERENCE

%\subsection{What causes this?} Embedding

\section{Experiment 2: Memory-Surprisal Tradeoff}

We now turn to a measure of memory load that does not depend on the syntactic analysis, namely the Memory-Surprisal Tradeoff \citep{hahn2019memory}.


Let $I_t$ be the Conditional Mutual Information of two words $X_0, X_t$ conditioned on the intervening words:
\begin{equation}
        \operatorname{I}_t := \operatorname{I}[X_t, X_0 | X_1, \dots, X_{t-1}] = \operatorname{H}[X_t|X_1, \dots, X_{t-1}] - \operatorname{H}[X_t|X_0, \dots, X_{t-1}] 
\end{equation}
This quantity, visualized in Figure~\ref{fig:theorem} (a), measures how much predictive information is provided by the next word $X_t$ by the word $t$ steps in the past.
It is a statistical property of the language, and can be estimated from large-scale text data.
Our theoretical results describe how the tradeoff between memory and comprehension difficulty relates to $I_t$ (Figure~\ref{fig:theorem} (b)):
Consider a listener who invests $B$ bits of memory into representing past input.
We then consider the smallest $T$ such that the area under the curve of $t I_t$, to the left of $T$, has size $B$.
Such a listener will experience average surprisal at least $H[X_t| X_{<t}] + \sum_{t=T+1}^\infty I_t$. %\note{maybe result first, and then say what $T$ is}
By tracing out all values $T >0$, one can obtain a bound on the tradeoff curve for any possible listener.


\paragraph{Setup}
We estimated $I_t$ using standard estimation methods for language modeling based on neural networks~\citep{hochreiter-long-1997}, as described in \citet{hahn2019memory}.

\paragraph{Results}

compared

- real orders

- subject placed after aux etc.


test which words show the strongest difference


\section{Conclusion}
We examined the question whether the typological prevalence of SOV order constitutes a problem for the idea that crosslinguistic word order distributions reflect optimization for processing under memory constraints.
We found that this is not the case: Word orders found in English and Japanese are optimal for the respective languages.
The results highlight that predictions of efficiency principles for language structure have to be evaluated against specific data from languages, and that they can have language-specific consequences even in domains that are crosslinguistically relevant, such as basic word order.


\bibliography{literature}
%\bibliographystyle{acl_natbib}


\end{document}





\section{Experiment 1: Optimization}

recycle QP experiment

here present the optimized grammars side-by-side



\section{Experiment 3: What Factors are Responsible?}

- all NPs are strictly head-final, and have final case marker

- almost all verbs are `embedded'

-- either with auxiliary

-- or with something else



\begin{center}
\begin{tabular}{lllllllll}
                    &                        & O,S same side   & O, S different sides    \\ \hline\hline
Ordinary UD         & Plain                  & 53.34 & 53.09\\
                    & Reordering Siblings    & 45.99 & 48.02 \\ \hline
Removing FuncWords  & Plain                  & 23.85 & 23.53 \\
                    & Reordering Siblings    & 19.68 & 20.71 \\ \hline
FuncHead UD         & Plain                  & 51.81 & 53.24 \\ % (excluding aux)
                    & Reordering Siblings    &  TODO     &  \\ \hline
FuncHead UD+Aux         & Plain                  & 67.86 & 69.25 \\ % (including aux) TODO this faces the problem that stacked AUX might be analyzed in the wrong order
                    & Reordering Siblings    &  TODO     & \\  \hline 
\end{tabular}
\end{center}