\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{tikz-dependency}

  \usepackage{natbib}
  \bibliographystyle{plainnat}

\usepackage{CJKutf8}

%  \usepackage{natbib}
 % \bibliographystyle{plainnat}


\pagestyle{plain}

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

% Use the lineno option to display guide line numbers if required.

\usepackage{amsmath}
\usepackage{tikz-dependency}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
\usepackage{multirow}





\usepackage[T1]{fontenc}

\usepackage{pslatex}
%\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[colorinlistoftodos]{todonotes}
\usepackage{rotating}
%\usepackage{natbib}
\usepackage{amssymb}


\usepackage{amsthm}
 


\newcounter{theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{example}[theorem]{Example}
\newtheorem{defin}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{thm}[theorem]{Theorem}


\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\Ff}[0]{\mathcal{F}}
\newcommand{\key}[1]{\textbf{#1}}


\newcommand{\soft}[1]{}
\newcommand{\nopreview}[1]{}
\newcommand\comment[1]{{\color{red}#1}}
\newcommand\mhahn[1]{{\color{red}(#1)}}
\newcommand{\rljf}[1]{{\color{blue}[rljf: #1]}}

\newcommand{\thetad}[0]{{\theta_d}}
\newcommand{\thetal}[0]{{\theta_{LM}}}
\newcommand{\thetap}[0]{{\theta_{P}}}
\newcommand{\japanese}[1]{\begin{CJK}{UTF8}{min}#1\end{CJK}}


\title{Dependency Length Minimization and Basic Word Order in Japanese}
\author{Michael Hahn \\ Stanford University \\ mhahn2@stanford.edu}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}
DLM widely supported, but appears to be at odds with the high typological frequency of SOV and VSO orders.
We report a corpus-based study of Japanese that tests this.
\end{abstract}

{\color{red} stats/distribution for Exp1}

{\color{red} viz for Exp2}

{\color{red} more Jap examples}

\section{Introduction}

There has been a suggestion that the typological distribution of word orders reflects a pressure towards efficiency of on-line syntactic processing.

Prominent among these proposals is the idea that syntactic structures are easier to process if syntactically related words are closer together in the surface string.

\cite{rijkhoff-word-1986} proposed a principle of \emph{Head Proximity}, arguing that, among other things, head nouns tend to be close to the verbs governing them.
\cite{hawkins2014crosslinguistic} proposes a similar principle of \emph{Domain Minimization}, arguing that languages minimize the distances at which syntactic relations are established in a sentence.
The principle of \emph{Dependency Length Minimization} \citep{temperley2018minimizing} is a formalization of these ideas in terms of dependency grammar.

Dependency Length Minimization has been argued to explain Greenberg's harmonic correlations \citep{rijkhoff-word-1986, hawkins1994performance}, and certain kinds of exceptions, such as why pronominal objects pattern differently from nominal objects \citep{dryer1992greenbergian}.
Hawkins (\citep{hawkins2004efficiency, hawkins2007processing, hawkins2014crosslinguistic}) has assembled further evidence in favor of 

Despite the success of Dependency Length Minimization, it appears to be at odds with the prevalence of SOV order:

In a sentence such as

\begin{centering}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
          She \& met \& friends  \\
   \end{deptext}
        %   \deproot{3}{ROOT}
   \depedge{2}{1}{subject}
   \depedge{2}{3}{object}
   %\depedge[arc angle=50]{7}{6}{ATT}
\end{dependency}
\end{centering}

the overall dependency length is minimized by the English order, compared to the Japanese order

\begin{centering}
\begin{dependency}[theme = simple]
   \begin{deptext}[column sep=1em]
          \japanese{彼女は} \& \japanese{友達に} \& \japanese{会った}  \\
          kanojo-wa \& tomodachi-ni \& atta \\ 
          she-\textsc{subj} \& friends-\textsc{obj} \& met \\
   \end{deptext}
        %   \deproot{3}{ROOT}
   \depedge{3}{1}{subject}
   \depedge{3}{2}{object}
   %\depedge[arc angle=50]{7}{6}{ATT}
\end{dependency}
\end{centering}


This paper examines this claim.

First, does the above claim remain true if we consider sentences more complex than the three-word one above?
In particular, does it remain true if we consider the average dependency length over the distribution of sentences as found in a corpus?

We will address this question by comparing the dependency length of actual Japanese orderings with counterfactual orderings exhibiting SVO order.

Second, what is the situation if we consider a more direct formalization of online memory limitations?


\section{Experiment 1: Comparison of Dependency Length}

\subsection{Method}

%\paragraph{Japanese}
We based our experiments on the Universal Dependencies treebanks.
Following \cite{futrell2015largescale}, we preprocessed these by promoting functional words (cc, case, cop, mark) to heads.
We restricted the experiment to the training partitions of each treebank, leaving the held-out partitions for future confirmatory research.

Again following \cite{futrell2015largescale}, we measure the length of a dependency by the number of intervening words.
The overall dependency length of a sentence is the sum of the lengths of all dependencies in that sentence.

For each sentence in the treebanks, we compared its total dependency length to that of a version to which the following manipulation was applied:

Any subject, identified as the dependent of an \textit{nsubj} dependency, was moved to the right of its head.
Heads sometimes have other dependencies to its right side, in particular verbs often have \textit{aux} dependents on the right.
We ordered the subject after these dependents.
As subject NPs are mostly longer than auxiliaries (which always consist of one word), this method minimizes dependency length among all ways of placing the subject after the head, and thus biases \emph{against} our hypothesis that SOV worder is optimal for dependency length.

This manipulation results in a version of Japanese that has OVS order.
Since dependency length is left-right invariant, it equally corresponds to an SVO version of Japanese, in which all phrases are strictly head-initial, and only subjects (\textit{nsubj}) and auxiliaries (\textit{aux}) precede their heads.


\subsection{Results}
In Table~\ref{tab:depl-resu}, we show the results for the all three corpora.
In each of the corpora, the overall dependency length is lower on average in the original SOV orders than in counterfactual orders where subject and object are ordered on different sides of the verb.


\begin{table}
\begin{center}
\begin{tabular}{l|l|lllllll}
 Corpus                   &   Sents.                     & O,S same side   & O, S different sides    \\ \hline\hline
All Corpora     &  54K     & 61.65 & 62.66 \\ \hline
GSD             &   7K       & 51.64 &  52.92   \\
BCCWJ           &   41K       &    63.04   & 63.97 \\
KTC             &   6K   &  64.01 & 65.27 \\ \hline
\end{tabular}
\end{center}
\caption{Dependency Lengths for Japanese. For each corpus, we show the number of sentences, and average overall dependency length for the real orders (O, S on the same side of the verb), and the counterfactual orders (O, S on different sides of the verb).}\label{tab:depl-resu}
\end{table}


%TODO

%- COMPARISON TO ENGLISH

%- STATISTICAL INFERENCE

%\subsection{What causes this?} Embedding

\section{Experiment 2: Memory-Surprisal Tradeoff}

We now turn to a measure of memory load that does not depend on the syntactic analysis, namely the Memory-Surprisal Tradeoff \citep{hahn2019memory}.


Let $I_t$ be the Conditional Mutual Information of two words $X_0, X_t$ conditioned on the intervening words:
\begin{equation}
        \operatorname{I}_t := \operatorname{I}[X_t, X_0 | X_1, \dots, X_{t-1}] = \operatorname{H}[X_t|X_1, \dots, X_{t-1}] - \operatorname{H}[X_t|X_0, \dots, X_{t-1}] 
\end{equation}
This quantity, visualized in Figure~\ref{fig:theorem} (a), measures how much predictive information is provided by the next word $X_t$ by the word $t$ steps in the past.
It is a statistical property of the language, and can be estimated from large-scale text data.
Our theoretical results describe how the tradeoff between memory and comprehension difficulty relates to $I_t$ (Figure~\ref{fig:theorem} (b)):
Consider a listener who invests $B$ bits of memory into representing past input.
We then consider the smallest $T$ such that the area under the curve of $t I_t$, to the left of $T$, has size $B$.
Such a listener will experience average surprisal at least $H[X_t| X_{<t}] + \sum_{t=T+1}^\infty I_t$. %\note{maybe result first, and then say what $T$ is}
By tracing out all values $T >0$, one can obtain a bound on the tradeoff curve for any possible listener.


\paragraph{Setup}
We estimated $I_t$ using standard estimation methods for language modeling based on neural networks~\citep{hochreiter-long-1997}, as described in \citet{hahn2019memory}.

\paragraph{Results}

compared

- real orders

- subject placed after aux etc.


test which words show the strongest difference


\section{Conclusion}
We examined the question whether the typological prevalence of SOV order constitutes a problem for the idea that crosslinguistic word order distributions reflect optimization for processing under memory constraints.
We found that this is not the case: Word orders found in English and Japanese are optimal for the respective languages.
The results highlight that predictions of efficiency principles for language structure have to be evaluated against specific data from languages, and that they can have language-specific consequences even in domains that are crosslinguistically relevant, such as basic word order.


\bibliography{literature}
%\bibliographystyle{acl_natbib}


\end{document}





\section{Experiment 1: Optimization}

recycle QP experiment

here present the optimized grammars side-by-side



\section{Experiment 3: What Factors are Responsible?}

- all NPs are strictly head-final, and have final case marker

- almost all verbs are `embedded'

-- either with auxiliary

-- or with something else



\begin{center}
\begin{tabular}{lllllllll}
                    &                        & O,S same side   & O, S different sides    \\ \hline\hline
Ordinary UD         & Plain                  & 53.34 & 53.09\\
                    & Reordering Siblings    & 45.99 & 48.02 \\ \hline
Removing FuncWords  & Plain                  & 23.85 & 23.53 \\
                    & Reordering Siblings    & 19.68 & 20.71 \\ \hline
FuncHead UD         & Plain                  & 51.81 & 53.24 \\ % (excluding aux)
                    & Reordering Siblings    &  TODO     &  \\ \hline
FuncHead UD+Aux         & Plain                  & 67.86 & 69.25 \\ % (including aux) TODO this faces the problem that stacked AUX might be analyzed in the wrong order
                    & Reordering Siblings    &  TODO     & \\  \hline 
\end{tabular}
\end{center}


\begin{table}
\begin{tabular}{l|rlll}
Name & Words & Available\\ \hline
GSD     & 184K & full\\
PUD     & 26K & full\\
Modern & 14K & full\\
BCCWJ & 1,237K & POS+deps\\
KTC & 189K & POS+deps \\
\end{tabular}
\end{table}




2.4 total: 54K sentences

Functional Heads:

cc: \begin{CJK}{UTF8}{min}と, や\end{CJK}

case: doesn't apply to verbs\footnote{https://universaldependencies.org/ja/dep/case.html}

cop: \begin{CJK}{UTF8}{min}だ\end{CJK} 

mark: \begin{CJK}{UTF8}{min}と, か\end{CJK} 

Method for bringing S to the other side: sorting it after the other dependents, as these are short (auxiliaries etc).

\paragraph{English}